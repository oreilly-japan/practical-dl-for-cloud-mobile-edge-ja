{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-2/2-colab-what-does-my-neural-network-think.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/practicaldl/Practical-Deep-Learning-Book/blob/master/code/chapter-2/2-colab-what-does-my-neural-network-think.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "This code is part of [Chapter 2 - What’s in the Picture: Image Classification with Keras](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch02.html). This is a Colab-specific version. To run on Jupyter locally, please run <a target=\"_blank\" href=\"c\">chapter-2/2-what-does-my-neural-network-think.ipynb</a> instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IS_COLAB_ENV = True\n",
    "except:\n",
    "  IS_COLAB_ENV = False\n",
    "  print(\"Please execute the non-Colab version of this notebook.\")\n",
    "IS_COLAB_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0.0\n",
    "!pip install tf-explain==0.1.0\n",
    "!pip install -U pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json --output imagenet_class_index.json\n",
    "!curl https://raw.githubusercontent.com/PracticalDL/Practical-Deep-Learning-Book/master/sample-images/dog.jpg --output dog.jpg\n",
    "!curl https://raw.githubusercontent.com/PracticalDL/Practical-Deep-Learning-Book/master/sample-images/cat.jpg --output cat.jpg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import get_file\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "CLASS_INDEX = None\n",
    "CLASS_INDEX_PATH = 'imagenet_class_index.json'\n",
    "\n",
    "# Note: decode_predictions(preds, top) is originally a keras function.\n",
    "# We have modified it here so that it returns the index of the class label along with the predictions.\n",
    "# The results are assimilated based on the assumption that there is only one top 1% prediction.\n",
    "\n",
    "def decode_predictions_modified(preds, top=1):\n",
    "    global CLASS_INDEX\n",
    "    if len(preds.shape) != 2 or preds.shape[1] != 1000:\n",
    "        raise ValueError(\n",
    "            '`decode_predictions` expects ' 'a batch of predictions ''(i.e. a 2D array of shape (samples, 1000)). ' 'Found array with shape: ' + str(preds.shape))\n",
    "    if CLASS_INDEX is None:\n",
    "        CLASS_INDEX = json.load(open(CLASS_INDEX_PATH))\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        for i in top_indices:\n",
    "            results = [i, tuple(CLASS_INDEX[str(i)]), (pred[i],)]\n",
    "    return results\n",
    "\n",
    "# Function that takes an image and model and produces the predictions\n",
    "\n",
    "def get_predictions(img, model):\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model.predict(x)\n",
    "    return decode_predictions_modified(preds, top=1)\n",
    "\n",
    "# NOTE: These two functions are taken from Shao-Chuan Wang <shaochuan.wang AT gmail.com> \n",
    "# as per the copyright on http://code.activestate.com/recipes/577591-conversion-of-pil-image-and-numpy-array/\n",
    "\n",
    "\"\"\"\n",
    "   Copyright 2011 Shao-Chuan Wang <shaochuan.wang AT gmail.com>\n",
    "\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "    of this software and associated documentation files (the \"Software\"), to deal\n",
    "    in the Software without restriction, including without limitation the rights\n",
    "    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "    copies of the Software, and to permit persons to whom the Software is\n",
    "    furnished to do so, subject to the following conditions:\n",
    "\n",
    "    The above copyright notice and this permission notice shall be included in\n",
    "    all copies or substantial portions of the Software.\n",
    "\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "    THE SOFTWARE.\n",
    "\"\"\"\n",
    "# Modified from the original format to take only the array as input and calculate the size on the fly\n",
    "def array_to_PIL(arr):\n",
    "    mode = 'RGBA'\n",
    "    # Use only the height and width for further processing\n",
    "    size = (arr.shape[0], arr.shape[1])  # (224,224)\n",
    "    arr = arr.reshape(arr.shape[0]*arr.shape[1], arr.shape[2])\n",
    "    if len(arr[0]) == 3:\n",
    "        arr = np.c_[arr, 255*np.ones((len(arr), 1), np.uint8)]\n",
    "    return Image.frombuffer(mode, size, arr.tostring(), 'raw', mode, 0, 1)\n",
    "\n",
    "# Function that puts text based prediction and class name on top of the image\n",
    "def overlay_prediction_on_image(img, prediction_class, prediction_probability, width, height):\n",
    "    img = img.resize((width, height), Image.ANTIALIAS)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    l = len(prediction_class)\n",
    "    # Place a black rectangle to provide a background for the text\n",
    "    # The size of the rectangle should change with respect to the image\n",
    "    draw.rectangle([int(width*0.05), int(width*0.05),\n",
    "                    int(width*0.5), int(width*0.11)], fill=(0, 0, 0))\n",
    "    draw.text((int(width*0.06), int(width*0.06)), '{0:.0f}'.format(\n",
    "        prediction_probability) + \"% \" + prediction_class, fill=(255, 255, 255))\n",
    "    return img\n",
    "\n",
    "# Based on StackOverflow code by user DTing\n",
    "# https://stackoverflow.com/questions/30227466/combine-several-images-horizontally-with-python\n",
    "\n",
    "def join_images(img1, img2):\n",
    "    widths, heights = zip(*(i.size for i in [img1, img2]))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "    new_img = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for img in [img1, img2]:\n",
    "        new_img.paste(img, (x_offset, 0))\n",
    "        x_offset += img.size[0]\n",
    "    return new_img\n",
    "\n",
    "def process_image(image_path, output_path):\n",
    "    model = VGG16(weights='imagenet', include_top=True, input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "    explainer = GradCAM()\n",
    "\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    data = ([img], None)\n",
    "\n",
    "    original_image = Image.open(image_path)\n",
    "    width = int(original_image.size[0]/4)\n",
    "    height = int(original_image.size[1]/4)\n",
    "    original_image.thumbnail((width, height), Image.ANTIALIAS)\n",
    "\n",
    "    class_index, class_name, prob_value = get_predictions(img, model)\n",
    "    heatmap = explainer.explain(data, model, \"block5_conv3\", class_index)\n",
    "\n",
    "    # overlay the text prediction on the heatmap overlay\n",
    "    heatmap_with_prediction_overlayed = overlay_prediction_on_image(\n",
    "        array_to_PIL(heatmap), class_name[-1], prob_value[0] * 100, width, height)\n",
    "\n",
    "    # place the images side by side\n",
    "    joined_image = join_images(\n",
    "        original_image, heatmap_with_prediction_overlayed)\n",
    "    joined_image.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Does My Neural Network Think?\n",
    "\n",
    "In this code sample, we try to understand why the neural network made a particular prediction. We use visualization (a heatmap) to understand the decision-making that is going on within the network. Using color, we visually identify the areas within an image that prompted a decision. “Hot” spots, represented by warmer colors (red, orange, and yellow) highlight the areas with the maximum signal, whereas cooler colors (blue, purple) indicate low signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `visualization.py` script produces the heatmap for one or more input images, overlays it on the image, and stitches it side-by-side with the original image for comparison. The script accepts arguments for image path or a directory that contains frames of a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Heatmap of an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_image(\"dog.jpg\", \"dog_output.jpg\")\n",
    "process_image(\"cat.jpg\", \"cat_output.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('dog_output.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('cat_output.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![t](./data/dog-output.jpg)\n",
    "The right half of the image indicates the “areas of heat” along with the correct prediction of a 'Cardigan Welsh Corgi'.\n",
    "\n",
    "Note: As we can see below, the label is different from the labels shown in the book. This is because we use the [VGG-19](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) model in the visualization script, whereas we used the [ResNet-50](https://github.com/KaimingHe/deep-residual-networks) model in the book.\n",
    "\n",
    "![t](./data/cat-output.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
